{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "957c524b",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fc39b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import prawcore\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from praw import Reddit\n",
    "from praw.models import MoreComments\n",
    "from typing import Union\n",
    "\n",
    "# Set Environment Vars\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afb8353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First time in South Korea\n",
      "------------------------------------\n",
      "It was my first time in South Korea and I‚Äôve been to a couple of places.\n",
      "\n",
      "\n",
      "My favorites would be Yangjae citizen forest (almost no crowd on a weekday), Eunpyeong Hanok Village (a bit far but, nice temperatures, quiet and beautiful sceneries)\n",
      "\n",
      "And thanks to a friend, I was able to go inside Hanam UN Village (Hanamdong UN Village hill yeah üéµ) the most(?) expensive place in Gangnam\n",
      "\n",
      "I like Suwon as well.\n",
      "\n",
      "\n",
      "Things I noticed was: it is incredibly quiet (sometimes I hear nothing I thought I got deaf)\n",
      "\n",
      "There are mirrors everywhere.\n",
      "\n",
      "There are couples EVERYWHERE.\n",
      "\n",
      "Food is great, but eating out means shelling out, usually a minimum of 10,000 won (and that is not cheap coming from a third world country)\n",
      "\n",
      "Subway can be confusing, don‚Äôt worry, even \n",
      "the locals get lost üòÇ, plus there are helpful people wearing red vest to help you. \n",
      "\n",
      "It can get overwhelming, I felt like I was bombarded by ads wherever I go, it is quiet with regards to noise, but it is visually overwhelming when you‚Äôre in the cities.\n",
      "\n",
      "There are sooooo many restos and cafes in the tourist heavy spots so it can be hard to choose, but it can also be an adventure, we still had great food even if we just went inside a resto with not much thought.\n",
      "---------------------------\n",
      "Also, I managed to see cherry blossoms and snow at the same time which my friends said was a really unusual and rare event\n",
      "/r/koreatravel/comments/1k423d7/first_time_in_south_korea/\n"
     ]
    }
   ],
   "source": [
    "reddit = init_reddit()\n",
    "post = next(reddit.subreddit(\"koreatravel\").top(limit=1))\n",
    "print(post.title)\n",
    "print(\"------------------------------------\")\n",
    "print(post.selftext.strip())\n",
    "print(\"---------------------------\")\n",
    "post.comments.replace_more(limit=0) # replace_more(limit=0) prevents getting more comments that are yet to be fetched. We just need the best comments.\n",
    "comments = [c for c in post.comments if not isinstance(c, MoreComments)]\n",
    "highest_score_comment = max(comments, key=lambda c: c.score).body.strip()\n",
    "print(highest_score_comment)\n",
    "print(post.permalink)\n",
    "# post.comments.replace_more(limit=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e8d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_reddit() -> None:\n",
    "    return Reddit(\n",
    "        client_id=os.environ[\"REDDIT_CLIENT_ID\"],\n",
    "        client_secret=os.environ[\"REDDIT_CLIENT_SECRET\"],\n",
    "        user_agent=os.environ[\"REDDIT_USER_AGENT\"],\n",
    "    )\n",
    "\n",
    "def clean_text(txt: str) -> str:\n",
    "    # strip HTML/Markdown\n",
    "    txt = BeautifulSoup(txt, \"html.parser\").get_text()\n",
    "    # remove code fences\n",
    "    txt = re.sub(r\"```[\\s\\S]*?```\", \"\", txt)\n",
    "    # collapse whitespace\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "def scrape(sub_size_map):\n",
    "    reddit = init_reddit()\n",
    "    qa = [] # the Q/A posts to train the model\n",
    "\n",
    "    for sub, sub_size in sub_size_map.items():\n",
    "        got_size = 0\n",
    "        for post in reddit.subreddit(sub).hot(limit=None): # , time_filter=\"all\"\n",
    "            # don't need to scrape more if got_size already matches sub_size\n",
    "            if got_size >= sub_size:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                # skip any link or image post, no posts with score lower than 1, no pinned/mod posts, ban any ‚Äúover 18‚Äù content, no locked thread, no crossposts\n",
    "                if (not post.is_self or post.removed_by_category\n",
    "                    or post.score < 1 or post.stickied or post.over_18\n",
    "                    or post.locked or hasattr(post, \"crosspost_parent\")):\n",
    "                    continue\n",
    "                \n",
    "                # if (post.removed_by_category or post.score < 1 \n",
    "                #     or post.stickied or post.over_18\n",
    "                #     or post.locked or hasattr(post, \"crosspost_parent\")):\n",
    "                #     continue\n",
    "                \n",
    "                # get the question as a merge of the title and body of the post\n",
    "                title = post.title.strip()\n",
    "                body = post.selftext.strip()\n",
    "                q = \"\\n\\n\".join(filter(None, [title, body]))\n",
    "                \n",
    "                # get the answer as the highest sore comment\n",
    "                post.comments.replace_more(limit=0) # replace_more(limit=0) prevents getting more comments that are yet to be fetched. We just need the best comments.\n",
    "                comments = post.comments.list()\n",
    "                if not comments: # if no comments at all, we can't create a Q/A pair dataset\n",
    "                    continue\n",
    "                top_comment = max(comments, key=lambda c: c.score)            \n",
    "                if top_comment.score < 1: # exclude posts with comments under 2 upvotes \n",
    "                    continue\n",
    "                a = top_comment.body.strip()\n",
    "\n",
    "                # length sanitation\n",
    "                if len(q.split()) < 3 or len(a.split()) < 6:\n",
    "                    continue\n",
    "                \n",
    "                qa.append({\n",
    "                    \"id\": post.id,\n",
    "                    \"subreddit\": sub,\n",
    "                    \"question\": q,\n",
    "                    \"answer\": a,\n",
    "                    \"url\": f\"https://reddit.com{post.permalink}\"\n",
    "                })\n",
    "                got_size += 1\n",
    "                \n",
    "            except prawcore.exceptions.TooManyRequests as e:\n",
    "                # reddit tells you how many seconds to wait\n",
    "                retry_after = int(e.response.headers.get(\"Retry-After\", 60))\n",
    "                print(f\"Rate limited‚Äîsleeping {retry_after}s...\")\n",
    "                time.sleep(retry_after)\n",
    "                # and then retry the same post\n",
    "                continue\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping post {post.id} due to {type(e).__name__}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        print(f\"- collected {got_size}/{sub_size} samples from r/{sub}\")\n",
    "    \n",
    "    return qa\n",
    "\n",
    "def preprocess(qa_raw):\n",
    "    cleaned = []\n",
    "    for item in qa_raw:\n",
    "        q = clean_text(item[\"question\"])\n",
    "        a = clean_text(item[\"answer\"])\n",
    "\n",
    "        cleaned.append({\n",
    "            \"question\": q,\n",
    "            \"answer\": a,\n",
    "            \"subreddit\": item[\"subreddit\"],\n",
    "            \"url\": item[\"url\"],\n",
    "        })\n",
    "    return cleaned\n",
    "\n",
    "def split_and_save(df, out_dir: Union[str, Path]):\n",
    "    # create the dir path if not existing\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # randomize the df rows, and reset to a fresh index(and droping the old one)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    n = len(df)\n",
    "    train_end = int(n * 0.75)\n",
    "    val_end   = train_end + int(n * 0.15)\n",
    "\n",
    "    splits = {\n",
    "        \"train\": df.iloc[:train_end],\n",
    "        \"val\":   df.iloc[train_end:val_end],\n",
    "        \"test\":  df.iloc[val_end:]\n",
    "    }\n",
    "    \n",
    "    for name, split_df in splits.items():\n",
    "        path = os.path.join(out_dir, f\"{name}.csv\")\n",
    "        split_df.to_csv(path, index=False)\n",
    "        print(f\"Saved {name} set: {len(split_df)} examples -> {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32871389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 1000 posts from 9 subreddits...\n",
      "- collected 400/400 samples from r/koreatravel\n",
      "- collected 50/50 samples from r/korea\n",
      "- collected 50/50 samples from r/southkorea\n",
      "- collected 150/150 samples from r/seoul\n",
      "- collected 100/100 samples from r/Living_in_Korea\n",
      "- collected 100/100 samples from r/solotravel\n",
      "- collected 50/50 samples from r/digitalnomad\n",
      "- collected 50/50 samples from r/askscience\n",
      "- collected 50/50 samples from r/AskHistorians\n",
      "Scraped 1000 raw Q&A; cleaning...\n",
      "Kept 1000 after cleaning; splitting...\n",
      "Saved train set: 750 examples -> /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/data/processed/train.csv\n",
      "Saved val set: 150 examples -> /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/data/processed/val.csv\n",
      "Saved test set: 100 examples -> /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/data/processed/test.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "RAW_DIR       = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# sub_size_map = {\n",
    "#     \"AskScience\": 16,\n",
    "#     \"explainlikeimfive\": 4,\n",
    "# }\n",
    "\n",
    "# sub_size_map = {\n",
    "#     \"koreatravel\": 20,\n",
    "#     \"travel\": 20,\n",
    "#     \"seoul\": 30,\n",
    "#     \"koreanfood\": 20,\n",
    "#     \"korea\": 20,\n",
    "#     \"solotravel\": 20,\n",
    "#     \"digitalnomad\": 20,\n",
    "# }\n",
    "\n",
    "sub_size_map = {\n",
    "    \"koreatravel\": 400,\n",
    "    \"korea\": 50,\n",
    "    \"southkorea\": 50,\n",
    "    # \"travel\": 100,\n",
    "    \"seoul\": 150,\n",
    "    \"Living_in_Korea\": 100,\n",
    "    \"solotravel\": 100,\n",
    "    \"digitalnomad\": 50,\n",
    "    \"askscience\": 50,\n",
    "    \"AskHistorians\": 50,\n",
    "}\n",
    "\n",
    "out=PROCESSED_DIR\n",
    "\n",
    "print(f\"Scraping {sum(sub_size_map.values())} posts from {len(sub_size_map)} subreddits...\")\n",
    "raw = scrape(sub_size_map)\n",
    "print(f\"Scraped {len(raw)} raw Q&A; cleaning...\")\n",
    "cleaned = preprocess(raw)\n",
    "print(f\"Kept {len(cleaned)} after cleaning; splitting...\")\n",
    "df = pd.DataFrame(cleaned)\n",
    "split_and_save(df, PROCESSED_DIR)\n",
    "\n",
    "# Save raw data JSON\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(RAW_DIR / \"qa_raw.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(raw, f, ensure_ascii=False, indent=2)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "984cc73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'remaining': 996.0, 'reset_timestamp': 1750972201.9752533, 'used': 4}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.auth.limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f341bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_reddit() -> None:\n",
    "    return Reddit(\n",
    "        client_id=os.environ[\"REDDIT_CLIENT_ID\"],\n",
    "        client_secret=os.environ[\"REDDIT_CLIENT_SECRET\"],\n",
    "        user_agent=os.environ[\"REDDIT_USER_AGENT\"],\n",
    "    )\n",
    "\n",
    "def clean_text(txt: str) -> str:\n",
    "    # strip HTML/Markdown\n",
    "    txt = BeautifulSoup(txt, \"html.parser\").get_text()\n",
    "    # remove code fences\n",
    "    txt = re.sub(r\"```[\\s\\S]*?```\", \"\", txt)\n",
    "    # collapse whitespace\n",
    "    return re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "\n",
    "def scrape(subreddits, sample_size):\n",
    "    reddit = init_reddit()\n",
    "    qa = [] # our Q/A posts to train the model\n",
    "    per_sub = (sample_size // len(subreddits)) + 1 # per sub sample_size with a safety buffer of 1\n",
    "\n",
    "    for sub in subreddits:\n",
    "        for post in reddit.subreddit(sub).hot(limit=per_sub * 2):\n",
    "            # don't need to scrape more if qa already matches sample_size\n",
    "            if len(qa) >= sample_size:\n",
    "                break\n",
    "            \n",
    "            # get the question as a merge of the title and body of the post\n",
    "            title = post.title.strip()\n",
    "            body = post.selftext.strip()\n",
    "            q = f\"{title}\\n\\n{body}\"\n",
    "            \n",
    "            # get the answer as the highest sore comment\n",
    "            post.comments.replace_more(limit=0) # replace_more(limit=0) prevents getting more comments that are yet to be fetched. We just need the best comments.\n",
    "            comments = [c for c in post.comments if not isinstance(c, MoreComments)]\n",
    "            if not comments: # if no comments at all, we can't create a Q/A pair dataset\n",
    "                continue\n",
    "            highest_score_a = max(comments, key=lambda c: c.score).body.strip()\n",
    "\n",
    "            # quick length sanitation\n",
    "            if len(q.split()) < 5 or len(highest_score_a.split()) < 10:\n",
    "                continue\n",
    "\n",
    "            qa.append({\n",
    "                \"id\": post.id,\n",
    "                \"subreddit\": sub,\n",
    "                \"question\": q,\n",
    "                \"answer\": highest_score_a,\n",
    "                \"url\": f\"https://reddit.com{post.permalink}\"\n",
    "            })\n",
    "        if len(qa) >= sample_size:\n",
    "            break\n",
    "\n",
    "    return qa[:sample_size]\n",
    "\n",
    "def preprocess(qa_raw):\n",
    "    cleaned = []\n",
    "    for item in qa_raw:\n",
    "        q = clean_text(item[\"question\"])\n",
    "        a = clean_text(item[\"answer\"])\n",
    "\n",
    "        cleaned.append({\n",
    "            \"question\": q,\n",
    "            \"answer\": a,\n",
    "            \"subreddit\": item[\"subreddit\"],\n",
    "            \"url\": item[\"url\"],\n",
    "        })\n",
    "    return cleaned\n",
    "\n",
    "def split_and_save(df, out_dir: Union[str, Path]):\n",
    "    # create the dir path if not existing\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # randomize the df rows, and reset to a fresh index(and droping the old one)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    n = len(df)\n",
    "    train_end = int(n * 0.75)\n",
    "    val_end   = train_end + int(n * 0.15)\n",
    "\n",
    "    splits = {\n",
    "        \"train\": df.iloc[:train_end],\n",
    "        \"val\":   df.iloc[train_end:val_end],\n",
    "        \"test\":  df.iloc[val_end:]\n",
    "    }\n",
    "    \n",
    "    for name, split_df in splits.items():\n",
    "        path = os.path.join(out_dir, f\"{name}.csv\")\n",
    "        split_df.to_csv(path, index=False)\n",
    "        print(f\"Saved {name} set: {len(split_df)} examples -> {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc70bd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 50 posts from 2 subreddits...\n",
      "Scraped 50 raw Q&A; cleaning...\n",
      "Kept 50 after cleaning; splitting...\n",
      "Saved train set: 37 examples -> /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/data/processed/train.csv\n",
      "Saved val set: 7 examples -> /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/data/processed/val.csv\n",
      "Saved test set: 6 examples -> /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/data/processed/test.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "RAW_DIR       = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    total=50,\n",
    "    subs=[\"explainlikeimfive\", \"AskScience\"],\n",
    "    out=PROCESSED_DIR\n",
    ")\n",
    "\n",
    "print(f\"Scraping {args.total} posts from {len(args.subs)} subreddits...\")\n",
    "raw = scrape(args.subs, args.total)\n",
    "print(f\"Scraped {len(raw)} raw Q&A; cleaning...\")\n",
    "cleaned = preprocess(raw)\n",
    "print(f\"Kept {len(cleaned)} after cleaning; splitting...\")\n",
    "df = pd.DataFrame(cleaned)\n",
    "split_and_save(df, args.out)\n",
    "\n",
    "# Save raw data JSON\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(RAW_DIR / \"qa_raw.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(raw, f, ensure_ascii=False, indent=2)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05260124",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4702a2",
   "metadata": {},
   "source": [
    "## Sample Dataset for Smoke Tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05260124",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fc39b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import prawcore\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from praw import Reddit\n",
    "from praw.models import MoreComments\n",
    "from typing import Union\n",
    "\n",
    "# Set Environment Vars\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75437830",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scripts_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m      2\u001b[0m project_root \u001b[38;5;241m=\u001b[39m scripts_dir\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m      3\u001b[0m project_root\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "scripts_dir = Path().resolve()\n",
    "project_root = scripts_dir.parent\n",
    "\n",
    "example_file_dir = scripts_dir.parent / \"example/example_file\"\n",
    "example_file_dir\n",
    "\n",
    "example_file = Path(example_file_dir)\n",
    "example_file\n",
    "\n",
    "example_file.mkdir(parents=True, exist_ok=True)\n",
    "example_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e8d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_reddit() -> None:\n",
    "    return Reddit(\n",
    "        client_id=os.environ[\"REDDIT_CLIENT_ID\"],\n",
    "        client_secret=os.environ[\"REDDIT_CLIENT_SECRET\"],\n",
    "        user_agent=os.environ[\"REDDIT_USER_AGENT\"],\n",
    "    )\n",
    "\n",
    "def clean_text(txt: str) -> str:\n",
    "    # strip HTML/Markdown\n",
    "    txt = BeautifulSoup(txt, \"html.parser\").get_text()\n",
    "    # remove code fences\n",
    "    txt = re.sub(r\"```[\\s\\S]*?```\", \"\", txt)\n",
    "    # collapse whitespace\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "def scrape(sub_size_map):\n",
    "    reddit = init_reddit()\n",
    "    qa = [] # the Q/A posts to train the model\n",
    "\n",
    "    for sub, sub_size in sub_size_map.items():\n",
    "        got_size = 0\n",
    "        for post in reddit.subreddit(sub).hot(limit=None): # , time_filter=\"all\"\n",
    "            # don't need to scrape more if got_size already matches sub_size\n",
    "            if got_size >= sub_size:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                # skip any link or image post, no posts with score lower than 1, no pinned/mod posts, ban any ‚Äúover 18‚Äù content, no locked thread, no crossposts\n",
    "                if (not post.is_self or post.removed_by_category\n",
    "                    or post.score < 1 or post.stickied or post.over_18\n",
    "                    or post.locked or hasattr(post, \"crosspost_parent\")):\n",
    "                    continue\n",
    "                \n",
    "                # if (post.removed_by_category or post.score < 1 \n",
    "                #     or post.stickied or post.over_18\n",
    "                #     or post.locked or hasattr(post, \"crosspost_parent\")):\n",
    "                #     continue\n",
    "                \n",
    "                # get the question as a merge of the title and body of the post\n",
    "                title = post.title.strip()\n",
    "                body = post.selftext.strip()\n",
    "                q = \"\\n\\n\".join(filter(None, [title, body]))\n",
    "                \n",
    "                # get the answer as the highest sore comment\n",
    "                post.comments.replace_more(limit=0) # replace_more(limit=0) prevents getting more comments that are yet to be fetched. We just need the best comments.\n",
    "                comments = post.comments.list()\n",
    "                if not comments: # if no comments at all, we can't create a Q/A pair dataset\n",
    "                    continue\n",
    "                top_comment = max(comments, key=lambda c: c.score)            \n",
    "                if top_comment.score < 1: # exclude posts with comments under 2 upvotes \n",
    "                    continue\n",
    "                a = top_comment.body.strip()\n",
    "\n",
    "                # length sanitation\n",
    "                if len(q.split()) < 3 or len(a.split()) < 6:\n",
    "                    continue\n",
    "                \n",
    "                qa.append({\n",
    "                    \"id\": post.id,\n",
    "                    \"subreddit\": sub,\n",
    "                    \"question\": q,\n",
    "                    \"answer\": a,\n",
    "                    \"url\": f\"https://reddit.com{post.permalink}\"\n",
    "                })\n",
    "                got_size += 1\n",
    "                \n",
    "            except prawcore.exceptions.TooManyRequests as e:\n",
    "                # reddit tells you how many seconds to wait\n",
    "                retry_after = int(e.response.headers.get(\"Retry-After\", 60))\n",
    "                print(f\"Rate limited‚Äîsleeping {retry_after}s...\")\n",
    "                time.sleep(retry_after)\n",
    "                # and then retry the same post\n",
    "                continue\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping post {post.id} due to {type(e).__name__}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        print(f\"- collected {got_size}/{sub_size} samples from r/{sub}\")\n",
    "    \n",
    "    return qa\n",
    "\n",
    "def preprocess(qa_raw):\n",
    "    cleaned = []\n",
    "    for item in qa_raw:\n",
    "        q = clean_text(item[\"question\"])\n",
    "        a = clean_text(item[\"answer\"])\n",
    "\n",
    "        cleaned.append({\n",
    "            \"question\": q,\n",
    "            \"answer\": a,\n",
    "            \"subreddit\": item[\"subreddit\"],\n",
    "            \"url\": item[\"url\"],\n",
    "        })\n",
    "    return cleaned\n",
    "\n",
    "def split_and_save(df, out_dir: Union[str, Path]):\n",
    "    # create the dir path if not existing\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # randomize the df rows, and reset to a fresh index(and droping the old one)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    n = len(df)\n",
    "    train_end = int(n * 0.75)\n",
    "    val_end   = train_end + int(n * 0.15)\n",
    "\n",
    "    splits = {\n",
    "        \"train\": df.iloc[:train_end],\n",
    "        \"val\":   df.iloc[train_end:val_end],\n",
    "        \"test\":  df.iloc[val_end:]\n",
    "    }\n",
    "    \n",
    "    for name, split_df in splits.items():\n",
    "        path = os.path.join(out_dir, f\"{name}.csv\")\n",
    "        split_df.to_csv(path, index=False)\n",
    "        print(f\"Saved {name} set: {len(split_df)} examples -> {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32871389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 1000 posts from 9 subreddits...\n",
      "- collected 400/400 samples from r/koreatravel\n",
      "- collected 50/50 samples from r/korea\n",
      "- collected 50/50 samples from r/southkorea\n",
      "- collected 150/150 samples from r/seoul\n",
      "- collected 100/100 samples from r/Living_in_Korea\n",
      "- collected 100/100 samples from r/solotravel\n",
      "- collected 50/50 samples from r/digitalnomad\n",
      "- collected 50/50 samples from r/askscience\n",
      "- collected 50/50 samples from r/AskHistorians\n",
      "Scraped 1000 raw Q&A; cleaning...\n",
      "Kept 1000 after cleaning; splitting...\n",
      "Saved train set: 750 examples -> /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/data/processed/train.csv\n",
      "Saved val set: 150 examples -> /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/data/processed/val.csv\n",
      "Saved test set: 100 examples -> /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/data/processed/test.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "RAW_DIR       = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# sub_size_map = {\n",
    "#     \"AskScience\": 16,\n",
    "#     \"explainlikeimfive\": 4,\n",
    "# }\n",
    "\n",
    "# sub_size_map = {\n",
    "#     \"koreatravel\": 20,\n",
    "#     \"travel\": 20,\n",
    "#     \"seoul\": 30,\n",
    "#     \"koreanfood\": 20,\n",
    "#     \"korea\": 20,\n",
    "#     \"solotravel\": 20,\n",
    "#     \"digitalnomad\": 20,\n",
    "# }\n",
    "\n",
    "sub_size_map = {\n",
    "    \"koreatravel\": 400,\n",
    "    \"korea\": 50,\n",
    "    \"southkorea\": 50,\n",
    "    \"seoul\": 150,\n",
    "    \"Living_in_Korea\": 100,\n",
    "    \"solotravel\": 100,\n",
    "    \"digitalnomad\": 50,\n",
    "    \"askscience\": 50,\n",
    "    \"AskHistorians\": 50,\n",
    "}\n",
    "\n",
    "out=PROCESSED_DIR\n",
    "\n",
    "print(f\"Scraping {sum(sub_size_map.values())} posts from {len(sub_size_map)} subreddits...\")\n",
    "raw = scrape(sub_size_map)\n",
    "print(f\"Scraped {len(raw)} raw Q&A; cleaning...\")\n",
    "cleaned = preprocess(raw)\n",
    "print(f\"Kept {len(cleaned)} after cleaning; splitting...\")\n",
    "df = pd.DataFrame(cleaned)\n",
    "split_and_save(df, PROCESSED_DIR)\n",
    "\n",
    "# Save raw data JSON\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(RAW_DIR / \"qa_raw.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(raw, f, ensure_ascii=False, indent=2)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d97d2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 1000 raw Q&A; cleaning...\n",
      "Kept 1000 after cleaning; splitting...\n",
      "Saved train set: 800 examples -> /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/test-data/processed/train.csv\n",
      "Saved val set: 100 examples -> /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/test-data/processed/val.csv\n",
      "Saved test set: 100 examples -> /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/test-data/processed/test.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from korea_travel_guide.data import split_and_save, preprocess\n",
    "\n",
    "scripts_dir = Path().resolve()\n",
    "project_root = scripts_dir.parent\n",
    "\n",
    "raw_path = project_root / \"test-data/raw/qa_raw.json\"\n",
    "\n",
    "out_dir = project_root / \"test-data/processed\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2) Load it into memory\n",
    "with raw_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    raw = json.load(f)        # raw is now your list of {id, question, answer, ‚Ä¶}\n",
    "\n",
    "print(f\"Scraped {len(raw)} raw Q&A; cleaning...\")\n",
    "cleaned = preprocess(raw)\n",
    "\n",
    "print(f\"Kept {len(cleaned)} after cleaning; splitting...\")\n",
    "df = pd.DataFrame(cleaned)\n",
    "split_and_save(df, out_dir)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "984cc73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'remaining': 996.0, 'reset_timestamp': 1750972201.9752533, 'used': 4}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.auth.limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb8353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First time in South Korea\n",
      "------------------------------------\n",
      "It was my first time in South Korea and I‚Äôve been to a couple of places.\n",
      "\n",
      "\n",
      "My favorites would be Yangjae citizen forest (almost no crowd on a weekday), Eunpyeong Hanok Village (a bit far but, nice temperatures, quiet and beautiful sceneries)\n",
      "\n",
      "And thanks to a friend, I was able to go inside Hanam UN Village (Hanamdong UN Village hill yeah üéµ) the most(?) expensive place in Gangnam\n",
      "\n",
      "I like Suwon as well.\n",
      "\n",
      "\n",
      "Things I noticed was: it is incredibly quiet (sometimes I hear nothing I thought I got deaf)\n",
      "\n",
      "There are mirrors everywhere.\n",
      "\n",
      "There are couples EVERYWHERE.\n",
      "\n",
      "Food is great, but eating out means shelling out, usually a minimum of 10,000 won (and that is not cheap coming from a third world country)\n",
      "\n",
      "Subway can be confusing, don‚Äôt worry, even \n",
      "the locals get lost üòÇ, plus there are helpful people wearing red vest to help you. \n",
      "\n",
      "It can get overwhelming, I felt like I was bombarded by ads wherever I go, it is quiet with regards to noise, but it is visually overwhelming when you‚Äôre in the cities.\n",
      "\n",
      "There are sooooo many restos and cafes in the tourist heavy spots so it can be hard to choose, but it can also be an adventure, we still had great food even if we just went inside a resto with not much thought.\n",
      "---------------------------\n",
      "Also, I managed to see cherry blossoms and snow at the same time which my friends said was a really unusual and rare event\n",
      "/r/koreatravel/comments/1k423d7/first_time_in_south_korea/\n"
     ]
    }
   ],
   "source": [
    "reddit = init_reddit()\n",
    "post = next(reddit.subreddit(\"koreatravel\").top(limit=1))\n",
    "print(post.title)\n",
    "print(\"------------------------------------\")\n",
    "print(post.selftext.strip())\n",
    "print(\"---------------------------\")\n",
    "post.comments.replace_more(limit=0) # replace_more(limit=0) prevents getting more comments that are yet to be fetched. We just need the best comments.\n",
    "comments = [c for c in post.comments if not isinstance(c, MoreComments)]\n",
    "highest_score_comment = max(comments, key=lambda c: c.score).body.strip()\n",
    "print(highest_score_comment)\n",
    "print(post.permalink)\n",
    "# post.comments.replace_more(limit=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4702a2",
   "metadata": {},
   "source": [
    "## Sample Dataset for Smoke Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a606f41",
   "metadata": {},
   "source": [
    "## Tokenize and Reformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b386a9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsooh/ssd/t5/envs/conda_envs/travel/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged into Hugging Face Hub\n"
     ]
    }
   ],
   "source": [
    "from korea_travel_guide.utils import load_environ_vars\n",
    "from korea_travel_guide.data import tokenize_and_format\n",
    "from datasets import load_dataset\n",
    "\n",
    "load_environ_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5531767",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scripts_dir \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m()\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[1;32m      2\u001b[0m project_root \u001b[38;5;241m=\u001b[39m scripts_dir\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m      4\u001b[0m ds \u001b[38;5;241m=\u001b[39m load_dataset(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     data_files\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     }\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "scripts_dir = Path().resolve()\n",
    "project_root = scripts_dir.parent\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "      \"train\": str(project_root / \"data/processed/train.csv\"),\n",
    "      \"validation\": str(project_root / \"data/processed/val.csv\"),\n",
    "      \"test\": str(project_root / \"data/processed/test.csv\"),\n",
    "    }\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9a55c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tok, tok = tokenize_and_format(ds)\n",
    "ds_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708f3806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 33601, 328, 20280, 328, 38, 524, 883, 597, 36, 330, 24452, 1046, 43, 8, 4959, 11, 5188, 8697, 454, 1084, 1236, 21347, 4, 4356, 95, 259, 19, 127, 1041, 98, 38, 33, 10, 319, 9, 481, 86, 53, 38, 218, 17, 27, 90, 2489, 705, 605, 143, 964, 259, 25, 38, 2307, 62, 11, 5, 382, 328, 38, 64, 1994, 103, 449, 24452, 4, 546, 13, 4412, 1816, 964, 198, 127, 1046, 7, 213, 66, 3482, 50, 4835, 19, 328, 2], [0, 17312, 16752, 191, 10380, 16, 7789, 419, 8061, 10975, 17312, 16752, 191, 7, 1642, 296, 6, 2016, 10198, 1914, 149, 983, 47620, 13082, 640, 330, 1688, 13745, 1657, 1097, 16624, 4, 3548, 1344, 4, 175, 73, 2926, 73, 844, 1244, 12, 4124, 12, 1646, 73, 11535, 73, 37555, 73, 17312, 16752, 12, 4162, 12, 560, 12, 43230, 12, 23023, 12, 18888, 12, 9946, 7349, 12, 13364, 5182, 12, 11672, 12, 3583, 1397, 73, 1922, 32620, 3272, 322, 20, 1101, 16582, 9779, 4237, 36, 530, 5273, 43, 6126, 14, 5, 12034, 74, 9097, 1733, 2779, 5807, 6, 19, 1895, 1786, 11, 5, 4669, 3806, 148, 5, 1390, 137, 9592, 420, 144, 9, 5, 247, 30, 5, 220, 183, 4, 27672, 1588, 12257, 16, 421, 7, 1642, 198, 5996, 11, 5, 8095, 911, 9, 391, 20204, 2871, 1657, 8, 9925, 7488, 2376, 6, 172, 517, 88, 5, 2388, 9176, 443, 6, 18109, 13474, 17526, 8, 4669, 369, 9925, 19733, 30, 231, 181, 4, 119, 4, 4868, 4, 646, 24989, 1715, 47620, 13082, 640, 1401, 4, 611, 366, 879, 4, 175, 73, 47120, 73, 11535, 12, 225, 73, 844, 1244, 73, 4124, 73, 1366, 73, 42111, 245, 1301, 387, 725, 771, 401, 16271, 19058, 9763, 401, 104, 4377, 32288, 448, 1301, 406, 347, 6934, 43521, 224, 14, 42, 76, 17, 27, 29, 6154, 16752, 16, 7789, 13540, 9983, 7, 707, 360, 656, 12606, 87, 4505, 528, 7, 5, 16616, 6379, 2919, 9, 5, 369, 3073, 755, 4, 2], [0, 27710, 11, 6926, 260, 6, 2081, 17232, 219, 961, 328, 38, 437, 164, 15, 2081, 7, 6926, 260, 6, 7, 221, 687, 260, 496, 589, 4, 939, 437, 667, 7, 1955, 66, 2004, 8, 939, 437, 8020, 114, 35, 112, 4, 939, 197, 1095, 11, 10, 18344, 111, 141, 8414, 32, 5, 6401, 6, 1492, 2055, 141, 32, 5, 5351, 42254, 132, 4, 114, 45, 99, 6, 7656, 109, 939, 304, 7, 465, 2004, 38713, 244, 2968, 29, 46686, 211, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsooh/ssd/t5/envs/conda_envs/travel/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 6179, 59, 825, 24674, 2628, 443, 116, 2], [0, 1121, 1101, 6, 47, 218, 75, 486, 110, 247, 22, 530, 33594, 41667, 653, 18, 110, 477, 2230, 116, 280, 2370, 8, 2238, 32, 80, 430, 11991, 116, 3394, 74, 348, 802, 7586, 2], [0, 417, 8693, 40, 28, 18815, 8, 21084, 1973, 4, 2333, 5, 2737, 40, 889, 5, 1492, 15, 49, 998, 4, 14620, 5, 1842, 31, 2238, 114, 47, 240, 7, 4, 17668, 35, 259, 18, 5, 2178, 1842, 6, 24, 16, 11, 2370, 350, 35, 1205, 640, 417, 8693, 4, 642, 687, 260, 4, 1043, 4, 32059, 73, 417, 8693, 73, 428, 4311, 73, 8458, 2546, 73, 2619, 612, 33871, 114, 47, 236, 7, 697, 6152, 1493, 6, 356, 23, 1437, 46873, 15264, 21402, 45209, 7487, 15375, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[0, 33601, 328, 20280, 328, 38, 524, 883, 597, 36, 330, 24452, 1046, 43, 8, 4959, 11, 5188, 8697, 454, 1084, 1236, 21347, 4, 4356, 95, 259, 19, 127, 1041, 98, 38, 33, 10, 319, 9, 481, 86, 53, 38, 218, 17, 27, 90, 2489, 705, 605, 143, 964, 259, 25, 38, 2307, 62, 11, 5, 382, 328, 38, 64, 1994, 103, 449, 24452, 4, 546, 13, 4412, 1816, 964, 198, 127, 1046, 7, 213, 66, 3482, 50, 4835, 19, 328, 2], [0, 17312, 16752, 191, 10380, 16, 7789, 419, 8061, 10975, 17312, 16752, 191, 7, 1642, 296, 6, 2016, 10198, 1914, 149, 983, 47620, 13082, 640, 330, 1688, 13745, 1657, 1097, 16624, 4, 3548, 1344, 4, 175, 73, 2926, 73, 844, 1244, 12, 4124, 12, 1646, 73, 11535, 73, 37555, 73, 17312, 16752, 12, 4162, 12, 560, 12, 43230, 12, 23023, 12, 18888, 12, 9946, 7349, 12, 13364, 5182, 12, 11672, 12, 3583, 1397, 73, 1922, 32620, 3272, 322, 20, 1101, 16582, 9779, 4237, 36, 530, 5273, 43, 6126, 14, 5, 12034, 74, 9097, 1733, 2779, 5807, 6, 19, 1895, 1786, 11, 5, 4669, 3806, 148, 5, 1390, 137, 9592, 420, 144, 9, 5, 247, 30, 5, 220, 183, 4, 27672, 1588, 12257, 16, 421, 7, 1642, 198, 5996, 11, 5, 8095, 911, 9, 391, 20204, 2871, 1657, 8, 9925, 7488, 2376, 6, 172, 517, 88, 5, 2388, 9176, 443, 6, 18109, 13474, 17526, 8, 4669, 369, 9925, 19733, 30, 231, 181, 4, 119, 4, 4868, 4, 646, 24989, 1715, 47620, 13082, 640, 1401, 4, 611, 366, 879, 4, 175, 73, 47120, 73, 11535, 12, 225, 73, 844, 1244, 73, 4124, 73, 1366, 73, 42111, 245, 1301, 387, 725, 771, 401, 16271, 19058, 9763, 401, 104, 4377, 32288, 448, 1301, 406, 347, 6934, 43521, 224, 14, 42, 76, 17, 27, 29, 6154, 16752, 16, 7789, 13540, 9983, 7, 707, 360, 656, 12606, 87, 4505, 528, 7, 5, 16616, 6379, 2919, 9, 5, 369, 3073, 755, 4, 2], [0, 27710, 11, 6926, 260, 6, 2081, 17232, 219, 961, 328, 38, 437, 164, 15, 2081, 7, 6926, 260, 6, 7, 221, 687, 260, 496, 589, 4, 939, 437, 667, 7, 1955, 66, 2004, 8, 939, 437, 8020, 114, 35, 112, 4, 939, 197, 1095, 11, 10, 18344, 111, 141, 8414, 32, 5, 6401, 6, 1492, 2055, 141, 32, 5, 5351, 42254, 132, 4, 114, 45, 99, 6, 7656, 109, 939, 304, 7, 465, 2004, 38713, 244, 2968, 29, 46686, 211, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[0, 6179, 59, 825, 24674, 2628, 443, 116, 2], [0, 1121, 1101, 6, 47, 218, 75, 486, 110, 247, 22, 530, 33594, 41667, 653, 18, 110, 477, 2230, 116, 280, 2370, 8, 2238, 32, 80, 430, 11991, 116, 3394, 74, 348, 802, 7586, 2], [0, 417, 8693, 40, 28, 18815, 8, 21084, 1973, 4, 2333, 5, 2737, 40, 889, 5, 1492, 15, 49, 998, 4, 14620, 5, 1842, 31, 2238, 114, 47, 240, 7, 4, 17668, 35, 259, 18, 5, 2178, 1842, 6, 24, 16, 11, 2370, 350, 35, 1205, 640, 417, 8693, 4, 642, 687, 260, 4, 1043, 4, 32059, 73, 417, 8693, 73, 428, 4311, 73, 8458, 2546, 73, 2619, 612, 33871, 114, 47, 236, 7, 697, 6152, 1493, 6, 356, 23, 1437, 46873, 15264, 21402, 45209, 7487, 15375, 2]]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "ds_test = ds[\"train\"].select(range(3))\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model_inputs = tok(\n",
    "    ds_test[\"question\"],\n",
    "    max_length=1024,\n",
    "    truncation=True\n",
    ")\n",
    "print(model_inputs) # returns input_ids and attention_mask\n",
    "\n",
    "with tok.as_target_tokenizer():\n",
    "    labels = tok(\n",
    "        ds_test[\"answer\"],\n",
    "        max_length=256,\n",
    "        truncation=True\n",
    "    )\n",
    "print(labels) # returns input_ids and attention_mask\n",
    "\n",
    "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "print(model_inputs) # returns model_inputs with a new target column called \"labels\" with the answer input_ids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "512c0c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0, 33601,   328, 20280,   328,    38,   524,   883,   597,    36,\n",
       "           330, 24452,  1046,    43,     8,  4959,    11,  5188,  8697,   454,\n",
       "          1084,  1236, 21347,     4,  4356,    95,   259,    19,   127,  1041,\n",
       "            98,    38,    33,    10,   319,     9,   481,    86,    53,    38,\n",
       "           218,    17,    27,    90,  2489,   705,   605,   143,   964,   259,\n",
       "            25,    38,  2307,    62,    11,     5,   382,   328,    38,    64,\n",
       "          1994,   103,   449, 24452,     4,   546,    13,  4412,  1816,   964,\n",
       "           198,   127,  1046,     7,   213,    66,  3482,    50,  4835,    19,\n",
       "           328,     2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor([    0,  6179,    59,   825, 24674,  2628,   443,   116,     2])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tok[\"train\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

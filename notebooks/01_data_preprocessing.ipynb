{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "957c524b",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fc39b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import random\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from praw import Reddit\n",
    "from praw.models import MoreComments\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set Environment Vars\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3da1eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "def init_reddit():\n",
    "    return Reddit(\n",
    "        client_id=os.environ[\"REDDIT_CLIENT_ID\"],\n",
    "        client_secret=os.environ[\"REDDIT_CLIENT_SECRET\"],\n",
    "        user_agent=os.environ[\"REDDIT_USER_AGENT\"],\n",
    "    )\n",
    "\n",
    "def clean_text(txt: str) -> str:\n",
    "    # strip HTML/Markdown\n",
    "    txt = BeautifulSoup(txt, \"html.parser\").get_text()\n",
    "    # remove code fences\n",
    "    txt = re.sub(r\"```[\\s\\S]*?```\", \"\", txt)\n",
    "    # collapse whitespace\n",
    "    return re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "\n",
    "def scrape(subreddits, target):\n",
    "    reddit = init_reddit()\n",
    "    qa = []\n",
    "    per_sub = (target // len(subreddits)) + 1\n",
    "\n",
    "    for sub in subreddits:\n",
    "        for post in reddit.subreddit(sub).hot(limit=per_sub * 2):\n",
    "            if len(qa) >= target:\n",
    "                break\n",
    "\n",
    "            q = post.title.strip()\n",
    "            post.comments.replace_more(limit=0)\n",
    "            comments = [c for c in post.comments if not isinstance(c, MoreComments)]\n",
    "            if not comments:\n",
    "                continue\n",
    "\n",
    "            # pick highest-scoring\n",
    "            a = max(comments, key=lambda c: c.score).body.strip()\n",
    "\n",
    "            # quick length check\n",
    "            if len(q.split()) < 3 or len(a.split()) < 5:\n",
    "                continue\n",
    "\n",
    "            qa.append({\n",
    "                \"id\": post.id,\n",
    "                \"subreddit\": sub,\n",
    "                \"question\": q,\n",
    "                \"answer\": a,\n",
    "                \"url\": f\"https://reddit.com{post.permalink}\"\n",
    "            })\n",
    "        if len(qa) >= target:\n",
    "            break\n",
    "\n",
    "    return qa[:target]\n",
    "\n",
    "def preprocess(qa_raw):\n",
    "    cleaned = []\n",
    "    for item in qa_raw:\n",
    "        q = clean_text(item[\"question\"])\n",
    "        a = clean_text(item[\"answer\"])\n",
    "        # enforce word‐count bounds\n",
    "        if not (20 <= len(q.split()) <= 128):\n",
    "            continue\n",
    "        if not (20 <= len(a.split()) <= 256):\n",
    "            continue\n",
    "\n",
    "        cleaned.append({\n",
    "            \"question\": q,\n",
    "            \"answer\": a,\n",
    "            \"subreddit\": item[\"subreddit\"],\n",
    "            \"url\": item[\"url\"],\n",
    "        })\n",
    "    return cleaned\n",
    "\n",
    "def split_and_save(df, out_dir: Union[str, Path]):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    n = len(df)\n",
    "    train_end = int(n * 0.8)\n",
    "    val_end   = train_end + int(n * 0.1)\n",
    "\n",
    "    splits = {\n",
    "        \"train\": df.iloc[:train_end],\n",
    "        \"val\":   df.iloc[train_end:val_end],\n",
    "        \"test\":  df.iloc[val_end:]\n",
    "    }\n",
    "    # os.makedirs(out_dir, exist_ok=True)\n",
    "    for name, split_df in splits.items():\n",
    "        path = os.path.join(out_dir, f\"{name}.csv\")\n",
    "        split_df.to_csv(path, index=False)\n",
    "        print(f\"→ {name}: {len(split_df)} examples → {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc70bd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 50 posts from 2 subreddits…\n",
      "Scraped 50 raw Q&A; cleaning…\n",
      "Kept 8 after cleaning; splitting…\n",
      "→ train: 6 examples → /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/data/processed/train.csv\n",
      "→ val: 0 examples → /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/data/processed/val.csv\n",
      "→ test: 2 examples → /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/data/processed/test.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# assume notebook lives in project_root/notebooks/\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "RAW_DIR       = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    total=50,\n",
    "    subs=[\"explainlikeimfive\", \"AskScience\"],\n",
    "    out=PROCESSED_DIR\n",
    ")\n",
    "\n",
    "print(f\"Scraping {args.total} posts from {len(args.subs)} subreddits…\")\n",
    "raw = scrape(args.subs, args.total)\n",
    "print(f\"Scraped {len(raw)} raw Q&A; cleaning…\")\n",
    "cleaned = preprocess(raw)\n",
    "print(f\"Kept {len(cleaned)} after cleaning; splitting…\")\n",
    "df = pd.DataFrame(cleaned)\n",
    "split_and_save(df, args.out)\n",
    "# Optionally: save raw JSON\n",
    "# os.makedirs(\"data/raw\", exist_ok=True)\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(RAW_DIR / \"qa_raw.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(raw, f, ensure_ascii=False, indent=2)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05260124",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4702a2",
   "metadata": {},
   "source": [
    "## Sample Dataset for Smoke Tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

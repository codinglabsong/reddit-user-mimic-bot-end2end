{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "957c524b",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc39b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import prawcore\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from praw import Reddit\n",
    "from praw.models import MoreComments\n",
    "from typing import Union\n",
    "\n",
    "# Set Environment Vars\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afb8353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First time in South Korea\n",
      "------------------------------------\n",
      "It was my first time in South Korea and I‚Äôve been to a couple of places.\n",
      "\n",
      "\n",
      "My favorites would be Yangjae citizen forest (almost no crowd on a weekday), Eunpyeong Hanok Village (a bit far but, nice temperatures, quiet and beautiful sceneries)\n",
      "\n",
      "And thanks to a friend, I was able to go inside Hanam UN Village (Hanamdong UN Village hill yeah üéµ) the most(?) expensive place in Gangnam\n",
      "\n",
      "I like Suwon as well.\n",
      "\n",
      "\n",
      "Things I noticed was: it is incredibly quiet (sometimes I hear nothing I thought I got deaf)\n",
      "\n",
      "There are mirrors everywhere.\n",
      "\n",
      "There are couples EVERYWHERE.\n",
      "\n",
      "Food is great, but eating out means shelling out, usually a minimum of 10,000 won (and that is not cheap coming from a third world country)\n",
      "\n",
      "Subway can be confusing, don‚Äôt worry, even \n",
      "the locals get lost üòÇ, plus there are helpful people wearing red vest to help you. \n",
      "\n",
      "It can get overwhelming, I felt like I was bombarded by ads wherever I go, it is quiet with regards to noise, but it is visually overwhelming when you‚Äôre in the cities.\n",
      "\n",
      "There are sooooo many restos and cafes in the tourist heavy spots so it can be hard to choose, but it can also be an adventure, we still had great food even if we just went inside a resto with not much thought.\n",
      "---------------------------\n",
      "Also, I managed to see cherry blossoms and snow at the same time which my friends said was a really unusual and rare event\n",
      "/r/koreatravel/comments/1k423d7/first_time_in_south_korea/\n"
     ]
    }
   ],
   "source": [
    "reddit = init_reddit()\n",
    "post = next(reddit.subreddit(\"koreatravel\").top(limit=1))\n",
    "print(post.title)\n",
    "print(\"------------------------------------\")\n",
    "print(post.selftext.strip())\n",
    "print(\"---------------------------\")\n",
    "post.comments.replace_more(limit=0) # replace_more(limit=0) prevents getting more comments that are yet to be fetched. We just need the best comments.\n",
    "comments = [c for c in post.comments if not isinstance(c, MoreComments)]\n",
    "highest_score_comment = max(comments, key=lambda c: c.score).body.strip()\n",
    "print(highest_score_comment)\n",
    "print(post.permalink)\n",
    "# post.comments.replace_more(limit=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_reddit() -> None:\n",
    "    return Reddit(\n",
    "        client_id=os.environ[\"REDDIT_CLIENT_ID\"],\n",
    "        client_secret=os.environ[\"REDDIT_CLIENT_SECRET\"],\n",
    "        user_agent=os.environ[\"REDDIT_USER_AGENT\"],\n",
    "    )\n",
    "\n",
    "def clean_text(txt: str) -> str:\n",
    "    # strip HTML/Markdown\n",
    "    txt = BeautifulSoup(txt, \"html.parser\").get_text()\n",
    "    # remove code fences\n",
    "    txt = re.sub(r\"```[\\s\\S]*?```\", \"\", txt)\n",
    "    # collapse whitespace\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "def scrape(sub_size_map):\n",
    "    reddit = init_reddit()\n",
    "    qa = [] # the Q/A posts to train the model\n",
    "\n",
    "    for sub, sub_size in sub_size_map.items():\n",
    "        got_size = 0\n",
    "        for post in reddit.subreddit(sub).top(limit=None, time_filter=\"all\"):\n",
    "            # don't need to scrape more if got_size already matches sub_size\n",
    "            if got_size >= sub_size:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                # skip any link or image post, no posts with score lower than 1, no pinned/mod posts, ban any ‚Äúover 18‚Äù content, no locked thread, no crossposts\n",
    "                # if (not post.is_self or post.removed_by_category\n",
    "                #     or post.score < 1 or post.stickied or post.over_18\n",
    "                #     or post.locked or hasattr(post, \"crosspost_parent\")):\n",
    "                #     continue\n",
    "                \n",
    "                if (post.removed_by_category or post.score < 1 \n",
    "                    or post.stickied or post.over_18\n",
    "                    or post.locked or hasattr(post, \"crosspost_parent\")):\n",
    "                    continue\n",
    "                \n",
    "                # get the question as a merge of the title and body of the post\n",
    "                title = post.title.strip()\n",
    "                body = post.selftext.strip()\n",
    "                q = \"\\n\\n\".join(filter(None, [title, body]))\n",
    "                \n",
    "                # get the answer as the highest sore comment\n",
    "                post.comments.replace_more(limit=0) # replace_more(limit=0) prevents getting more comments that are yet to be fetched. We just need the best comments.\n",
    "                comments = post.comments.list()\n",
    "                if not comments: # if no comments at all, we can't create a Q/A pair dataset\n",
    "                    continue\n",
    "                top_comment = max(comments, key=lambda c: c.score)            \n",
    "                if top_comment.score < 1: # exclude posts with comments under 2 upvotes \n",
    "                    continue\n",
    "                a = top_comment.body.strip()\n",
    "\n",
    "                # length sanitation\n",
    "                if len(q.split()) < 3 or len(a.split()) < 5:\n",
    "                    continue\n",
    "                \n",
    "                qa.append({\n",
    "                    \"id\": post.id,\n",
    "                    \"subreddit\": sub,\n",
    "                    \"question\": q,\n",
    "                    \"answer\": a,\n",
    "                    \"url\": f\"https://reddit.com{post.permalink}\"\n",
    "                })\n",
    "                got_size += 1\n",
    "                \n",
    "            except prawcore.exceptions.TooManyRequests as e:\n",
    "                # reddit tells you how many seconds to wait\n",
    "                retry_after = int(e.response.headers.get(\"Retry-After\", 60))\n",
    "                print(f\"Rate limited‚Äîsleeping {retry_after}s...\")\n",
    "                time.sleep(retry_after)\n",
    "                # and then retry the same post\n",
    "                continue\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping post {post.id} due to {type(e).__name__}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        print(f\"- collected {got_size}/{sub_size} samples from r/{sub}\")\n",
    "    \n",
    "    return qa\n",
    "\n",
    "def preprocess(qa_raw):\n",
    "    cleaned = []\n",
    "    for item in qa_raw:\n",
    "        q = clean_text(item[\"question\"])\n",
    "        a = clean_text(item[\"answer\"])\n",
    "\n",
    "        cleaned.append({\n",
    "            \"question\": q,\n",
    "            \"answer\": a,\n",
    "            \"subreddit\": item[\"subreddit\"],\n",
    "            \"url\": item[\"url\"],\n",
    "        })\n",
    "    return cleaned\n",
    "\n",
    "def split_and_save(df, out_dir: Union[str, Path]):\n",
    "    # create the dir path if not existing\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # randomize the df rows, and reset to a fresh index(and droping the old one)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    n = len(df)\n",
    "    train_end = int(n * 0.75)\n",
    "    val_end   = train_end + int(n * 0.15)\n",
    "\n",
    "    splits = {\n",
    "        \"train\": df.iloc[:train_end],\n",
    "        \"val\":   df.iloc[train_end:val_end],\n",
    "        \"test\":  df.iloc[val_end:]\n",
    "    }\n",
    "    \n",
    "    for name, split_df in splits.items():\n",
    "        path = os.path.join(out_dir, f\"{name}.csv\")\n",
    "        split_df.to_csv(path, index=False)\n",
    "        print(f\"Saved {name} set: {len(split_df)} examples -> {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32871389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 2000 posts from 7 subreddits...\n",
      "- created 469 samples from r/koreatravel\n",
      "- created 300 samples from r/travel\n"
     ]
    },
    {
     "ename": "TooManyRequests",
     "evalue": "received 429 HTTP response",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m out\u001b[38;5;241m=\u001b[39mPROCESSED_DIR\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(sub_size_map\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m posts from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sub_size_map)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m subreddits...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m raw \u001b[38;5;241m=\u001b[39m \u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_size_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraped \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(raw)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m raw Q&A; cleaning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m cleaned \u001b[38;5;241m=\u001b[39m preprocess(raw)\n",
      "Cell \u001b[0;32mIn[74], line 31\u001b[0m, in \u001b[0;36mscrape\u001b[0;34m(sub_size_map)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# skip any link or image post, no posts with score lower than 1, no pinned/mod posts, ban any ‚Äúover 18‚Äù content, no locked thread, no crossposts\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m post\u001b[38;5;241m.\u001b[39mis_self \u001b[38;5;129;01mor\u001b[39;00m post\u001b[38;5;241m.\u001b[39mremoved_by_category\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m post\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m post\u001b[38;5;241m.\u001b[39mstickied \u001b[38;5;129;01mor\u001b[39;00m post\u001b[38;5;241m.\u001b[39mover_18\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m post\u001b[38;5;241m.\u001b[39mlocked \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrosspost_parent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# get the question as a merge of the title and body of the post\u001b[39;00m\n",
      "File \u001b[0;32m~/ssd/t5/envs/conda_envs/travel/lib/python3.10/site-packages/praw/models/reddit/base.py:38\u001b[0m, in \u001b[0;36mRedditBase.__getattr__\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the value of ``attribute``.\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attribute\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetched:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attribute)\n\u001b[1;32m     40\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattribute\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/ssd/t5/envs/conda_envs/travel/lib/python3.10/site-packages/praw/models/reddit/submission.py:726\u001b[0m, in \u001b[0;36mSubmission._fetch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fetch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 726\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m     submission_listing, comment_listing \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    728\u001b[0m     comment_listing \u001b[38;5;241m=\u001b[39m Listing(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reddit, _data\u001b[38;5;241m=\u001b[39mcomment_listing[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/ssd/t5/envs/conda_envs/travel/lib/python3.10/site-packages/praw/models/reddit/submission.py:744\u001b[0m, in \u001b[0;36mSubmission._fetch_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_additional_fetch_params\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m    743\u001b[0m path \u001b[38;5;241m=\u001b[39m API_PATH[name]\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfields)\n\u001b[0;32m--> 744\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reddit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ssd/t5/envs/conda_envs/travel/lib/python3.10/site-packages/praw/util/deprecate_args.py:46\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     arg_string \u001b[38;5;241m=\u001b[39m _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[1;32m     40\u001b[0m     warn(\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m     44\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_old_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ssd/t5/envs/conda_envs/travel/lib/python3.10/site-packages/praw/reddit.py:963\u001b[0m, in \u001b[0;36mReddit.request\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientException(msg)\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequest \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/ssd/t5/envs/conda_envs/travel/lib/python3.10/site-packages/prawcore/sessions.py:328\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    326\u001b[0m     json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m url \u001b[38;5;241m=\u001b[39m urljoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requestor\u001b[38;5;241m.\u001b[39moauth_url, path)\n\u001b[0;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ssd/t5/envs/conda_envs/travel/lib/python3.10/site-packages/prawcore/sessions.py:267\u001b[0m, in \u001b[0;36mSession._request_with_retries\u001b[0;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_retry(\n\u001b[1;32m    255\u001b[0m         data,\n\u001b[1;32m    256\u001b[0m         files,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m         url,\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTATUS_EXCEPTIONS:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTATUS_EXCEPTIONS[response\u001b[38;5;241m.\u001b[39mstatus_code](response)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m codes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_content\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTooManyRequests\u001b[0m: received 429 HTTP response"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "RAW_DIR       = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# sub_size_map = {\n",
    "#     \"AskScience\": 16,\n",
    "#     \"explainlikeimfive\": 4,\n",
    "# }\n",
    "\n",
    "# sub_size_map = {\n",
    "#     \"koreatravel\": 20,\n",
    "#     \"travel\": 20,\n",
    "#     \"seoul\": 30,\n",
    "#     \"koreanfood\": 20,\n",
    "#     \"korea\": 20,\n",
    "#     \"solotravel\": 20,\n",
    "#     \"digitalnomad\": 20,\n",
    "# }\n",
    "\n",
    "sub_size_map = {\n",
    "    \"koreatravel\": 500,\n",
    "    \"korea\": 200,\n",
    "    \"southkorea\": 100,\n",
    "    \"travel\": 200,\n",
    "    \"seoul\": 300,\n",
    "    \"Living_in_Korea\": 200,\n",
    "    \"solotravel\": 200,\n",
    "    \"digitalnomad\": 100,\n",
    "    \"askscience\": 100,\n",
    "    \"AskHistorians\": 100,\n",
    "}\n",
    "\n",
    "out=PROCESSED_DIR\n",
    "\n",
    "print(f\"Scraping {sum(sub_size_map.values())} posts from {len(sub_size_map)} subreddits...\")\n",
    "raw = scrape(sub_size_map)\n",
    "print(f\"Scraped {len(raw)} raw Q&A; cleaning...\")\n",
    "cleaned = preprocess(raw)\n",
    "print(f\"Kept {len(cleaned)} after cleaning; splitting...\")\n",
    "df = pd.DataFrame(cleaned)\n",
    "split_and_save(df, PROCESSED_DIR)\n",
    "\n",
    "# Save raw data JSON\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(RAW_DIR / \"qa_raw.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(raw, f, ensure_ascii=False, indent=2)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "984cc73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'remaining': 996.0, 'reset_timestamp': 1750972201.9752533, 'used': 4}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.auth.limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f341bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_reddit() -> None:\n",
    "    return Reddit(\n",
    "        client_id=os.environ[\"REDDIT_CLIENT_ID\"],\n",
    "        client_secret=os.environ[\"REDDIT_CLIENT_SECRET\"],\n",
    "        user_agent=os.environ[\"REDDIT_USER_AGENT\"],\n",
    "    )\n",
    "\n",
    "def clean_text(txt: str) -> str:\n",
    "    # strip HTML/Markdown\n",
    "    txt = BeautifulSoup(txt, \"html.parser\").get_text()\n",
    "    # remove code fences\n",
    "    txt = re.sub(r\"```[\\s\\S]*?```\", \"\", txt)\n",
    "    # collapse whitespace\n",
    "    return re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "\n",
    "def scrape(subreddits, sample_size):\n",
    "    reddit = init_reddit()\n",
    "    qa = [] # our Q/A posts to train the model\n",
    "    per_sub = (sample_size // len(subreddits)) + 1 # per sub sample_size with a safety buffer of 1\n",
    "\n",
    "    for sub in subreddits:\n",
    "        for post in reddit.subreddit(sub).hot(limit=per_sub * 2):\n",
    "            # don't need to scrape more if qa already matches sample_size\n",
    "            if len(qa) >= sample_size:\n",
    "                break\n",
    "            \n",
    "            # get the question as a merge of the title and body of the post\n",
    "            title = post.title.strip()\n",
    "            body = post.selftext.strip()\n",
    "            q = f\"{title}\\n\\n{body}\"\n",
    "            \n",
    "            # get the answer as the highest sore comment\n",
    "            post.comments.replace_more(limit=0) # replace_more(limit=0) prevents getting more comments that are yet to be fetched. We just need the best comments.\n",
    "            comments = [c for c in post.comments if not isinstance(c, MoreComments)]\n",
    "            if not comments: # if no comments at all, we can't create a Q/A pair dataset\n",
    "                continue\n",
    "            highest_score_a = max(comments, key=lambda c: c.score).body.strip()\n",
    "\n",
    "            # quick length sanitation\n",
    "            if len(q.split()) < 5 or len(highest_score_a.split()) < 10:\n",
    "                continue\n",
    "\n",
    "            qa.append({\n",
    "                \"id\": post.id,\n",
    "                \"subreddit\": sub,\n",
    "                \"question\": q,\n",
    "                \"answer\": highest_score_a,\n",
    "                \"url\": f\"https://reddit.com{post.permalink}\"\n",
    "            })\n",
    "        if len(qa) >= sample_size:\n",
    "            break\n",
    "\n",
    "    return qa[:sample_size]\n",
    "\n",
    "def preprocess(qa_raw):\n",
    "    cleaned = []\n",
    "    for item in qa_raw:\n",
    "        q = clean_text(item[\"question\"])\n",
    "        a = clean_text(item[\"answer\"])\n",
    "\n",
    "        cleaned.append({\n",
    "            \"question\": q,\n",
    "            \"answer\": a,\n",
    "            \"subreddit\": item[\"subreddit\"],\n",
    "            \"url\": item[\"url\"],\n",
    "        })\n",
    "    return cleaned\n",
    "\n",
    "def split_and_save(df, out_dir: Union[str, Path]):\n",
    "    # create the dir path if not existing\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # randomize the df rows, and reset to a fresh index(and droping the old one)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    n = len(df)\n",
    "    train_end = int(n * 0.75)\n",
    "    val_end   = train_end + int(n * 0.15)\n",
    "\n",
    "    splits = {\n",
    "        \"train\": df.iloc[:train_end],\n",
    "        \"val\":   df.iloc[train_end:val_end],\n",
    "        \"test\":  df.iloc[val_end:]\n",
    "    }\n",
    "    \n",
    "    for name, split_df in splits.items():\n",
    "        path = os.path.join(out_dir, f\"{name}.csv\")\n",
    "        split_df.to_csv(path, index=False)\n",
    "        print(f\"Saved {name} set: {len(split_df)} examples -> {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc70bd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 50 posts from 2 subreddits...\n",
      "Scraped 50 raw Q&A; cleaning...\n",
      "Kept 50 after cleaning; splitting...\n",
      "Saved train set: 37 examples -> /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/data/processed/train.csv\n",
      "Saved val set: 7 examples -> /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/data/processed/val.csv\n",
      "Saved test set: 6 examples -> /mnt/wsl/Dt5vhdx/projects/korea-travel-guide/data/processed/test.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "RAW_DIR       = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    total=50,\n",
    "    subs=[\"explainlikeimfive\", \"AskScience\"],\n",
    "    out=PROCESSED_DIR\n",
    ")\n",
    "\n",
    "print(f\"Scraping {args.total} posts from {len(args.subs)} subreddits...\")\n",
    "raw = scrape(args.subs, args.total)\n",
    "print(f\"Scraped {len(raw)} raw Q&A; cleaning...\")\n",
    "cleaned = preprocess(raw)\n",
    "print(f\"Kept {len(cleaned)} after cleaning; splitting...\")\n",
    "df = pd.DataFrame(cleaned)\n",
    "split_and_save(df, args.out)\n",
    "\n",
    "# Save raw data JSON\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(RAW_DIR / \"qa_raw.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(raw, f, ensure_ascii=False, indent=2)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05260124",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4702a2",
   "metadata": {},
   "source": [
    "## Sample Dataset for Smoke Tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
